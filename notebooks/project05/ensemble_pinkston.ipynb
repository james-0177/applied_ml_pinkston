{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0946ff",
   "metadata": {},
   "source": [
    "# Project 05: Ensemble Machine Learning - Wine Dataset\n",
    "**Author:**  James Pinkston  \n",
    "**Date:**  November 23, 2025  \n",
    "**Objective:**  P5:  This project will evaluate model performance by exploring ensemble models by using the Wine Quality Dataset located <a href=\"https://archive.ics.uci.edu/ml/datasets/Wine+Quality\" target=\"_blank\">here</a>.\n",
    "\n",
    "## Introduction to Ensemble Models\n",
    "Ensemble models combine the outputs of multiple models to improve predictive performance. Common types of ensemble models include:\n",
    "\n",
    "- Boosted Decision Trees – Models train sequentially, with each new tree correcting the errors of the previous one.\n",
    "- Random Forest – Multiple decision trees train in parallel, each on a random subset of the data, and their predictions are averaged.\n",
    "- Voting Classifier (Heterogeneous Models) – Combines different types of models (e.g., Decision Tree, SVM, and Neural Network) by taking the majority vote or average prediction.\n",
    "- Cross Validation – Divides data into multiple folds to improve the reliability of performance estimates.\n",
    "\n",
    "## Performance Metrics\n",
    "We will evaluate model performance using the following metrics:\n",
    "\n",
    "- Accuracy –  The proportion of all predictions that are correct.\n",
    "- Precision – Proportion of positive predictions that are truly positive.\n",
    "- Recall – Proportion of actual positives that are correctly predicted.\n",
    "- F1 Score – Harmonic mean of precision and recall, balancing both.\n",
    "\n",
    "These metrics are especially helpful when working with multiple classes (e.g., low, medium, high), not just binary yes/no predictions.\n",
    "\n",
    "Good models have:\n",
    "\n",
    "- High Test Accuracy – the model predicts well on new, unseen (e.g., test) data.\n",
    "- High Test F1 Score – especially useful  if some classes (categories) have fewer examples than others.\n",
    "- Small Gap between Train and Test accuracy – shows the model is generalizing well (not overfitting or underfitting).\n",
    "- Small Gap between Train and Test F1 score – shows the model is generalizing well (not overfitting or underfitting)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
